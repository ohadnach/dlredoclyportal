{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Getting Started  \n", "  \n", "This tutorial will help you get started with the basics of model management:  \n", "* logging metrics only (aka \u201coffline mode\u201d)  \n", "* downloading pretrained model for local training and inference  \n", "* deploying pretrained models from the AI library onto the Dataloop platform  \n", "  \n", "### Logging metrics  \n", "To export metrics for tracking model performance, you need to create a dummy package (with a dummy codebase reference) and model (including a valid dataset ID). Remember to replace <project_name> and <dataset_id> with the appropriate strings to reference your project and dataset.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "import os\n", "project = dl.projects.get(project_name='<project_id>')\n", "package = project.packages.push(package_name='dummy-model-package',\n", "                                codebase=dl.entities.LocalCodebase(os.getcwd()),\n", "                                modules=[])\n", "model = package.models.create(model_name='My Model',\n", "                              description='model for offline model logging',\n", "                              dataset_id='<dataset_id>',\n", "                              labels=[])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["  \n", "Once you\u2019ve created these two entities, metrics can be sent to the platform with the `model.add_log_samples` command.  \n", "Here is an example:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["epoch = np.linspace(0, 9, 10)\n", "epoch_metric = np.linspace(0, 9, 10)\n", "", "for x_metric, y_metric in zip(epoch, epoch_metric):\n", "    model.add_log_samples(samples=dl.LogSample(figure='tutorial plot',\n", "                                               legend='some metric',\n", "                                               x=x_metric,\n", "                                               y=y_metric),\n", "                          dataset_id=model.dataset_id)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Metrics plots will appear under the \u201cmetrics\u201d tab of your chosen model, and will look something like this:  \n", "![Screenshot of model metrics plot](https://github.com/dataloop-ai/dtlpy-documentation/blob/model_mgmt_3/assets/images/model_management/metrics_example.png/)  \n", "  \n", "### Using pretrained models from the AI library  \n", "  \n", "The Dataloop AI library includes various architectures and pretrained models that can be used for inference or further training.  \n", "  \n", "To see available public models, filter all available packages:  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["filters = dl.Filters(resource=dl.FiltersResource.PACKAGE, use_defaults=False)\n", "filters.add(field='scope', values='public')\n", "dl.packages.list(filters=filters).print()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Public models can be downloaded to your machine for local training and inference, or they can be trained and deployed on the cloud for integration into the Dataloop platform.  \n", "  \n", "### Dataset Subsets  \n", "Our public models use a train/validation split of the dataset for the training session. To avoid data leakage between training sessions and to make each training reproducible,  \n", "we will determine the data subsets and save the split type to the dataset entity (using a DQL). Using DQL filters you can subset the data however you like.  \n", "  \n", "For example, if your dataset is split between folders, you can use this DQL to add metadata for all items in the dataset  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["dataset.metadata['system']['subsets'] = {\n", "    'train': json.dumps(dl.Filters(field='dir', values='/train').prepare()),\n", "    'validation': json.dumps(dl.Filters(field='dir', values='/validation').prepare()),\n", "}\n", "dataset.update(system_metadata=True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This way, when the training starts, the sets will be downloaded using the DQL and any future training session on this dataset will have the same subsets of data.  \n", "  \n", "NOTE: In the future, this mechanism will be expanded to use a tagging system on items. This will allow more flexible data subsets and random data allocation.  \n", "  \n", "### Deploying a model remotely  \n", "  \n", "Download the model and package you want to copy it to your project. Since the public model is pretrained, it can be deployed without any further action.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["public_model = dl.models.get(model_id=\"<model_id>\")\n", "", "model = project.models.clone(from_model=public_model,\n", "                             model_name='remote_model',\n", "                             project_id=project.id)\n", "", "model.deploy()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you want to customize the public model (for transfer-learning or fine-tuning), you can indicate the new dataset and labels for model training.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["custom_model = dl.models.clone(from_model=public_model,\n", "                               model_name='remote_custom_model',\n", "                               dataset=dataset,\n", "                               project_id=project.id,\n", "                               labels=['label1', 'label2'])\n", "model.train()\n", "model.deploy()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A model can be trained with a new dataset with `model.train()`.  \n", "  \n", "A model can only be deployed after it's been trained. `model.deploy()` automatically creates a bot and service for the trained model.  \n", "  \n", "Now the model is deployed, you can create a UI slot to inference on individual data items on the platform, or call the model to inference in a FaaS.  \n", "  \n", "### Downloading a model for local training and inferencing  \n", "  \n", "For local training and inferecing, you need to download the package codebase to get build the model adapter that you need to run the train and predict methods. Follow the same steps as above to copy the public model to your project. Then download the model's package codebase to build and load the model adapter.  \n", "  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}