{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Dataset Binding with Azure  \n", "  \n", "We will create an Azure Function App to continuously sync a blob with Dataloop's dataset  \n", "  \n", "If you want to catch events from the Azure blob and update the Dataloop Dataset you need to set up a blob function.  \n", "The function will catch the blob storage events and will reflect them into the Dataloop Platform.  \n", "  \n", "If you are familiar with [Azure Function App](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python), you can just use our integration function below.  \n", "  \n", "We assume you already have an Azure account with resource group and storage account. If you don't, follow the [Azure docs](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create) and create them.  \n", "  \n", "### Create the Blob Function  \n", "1. Create a Container in the created Storage account  \n", "   * Public access level -> Container OR Blob  \n", "NOTE this container should be used as the external storage for the Dataloop dataset.  \n", "2. Go back to Resource group and click Create -> Function App  \n", "   * Choose Subscription, your Resource group, Name and Region  \n", "   * Publish -> Code  \n", "   * Runtime stack -> Python  \n", "   * Version -> <=3.7  \n", "  \n", "In VScode, flow the instructions in [azure docs](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python) to configure your environment and deploy the function:  \n", "1. Configure your environment  \n", "2. Sign in to Azure  \n", "3. Create your local project  \n", "   * in Select a template for your project's first function choose -> Azure Blob Storage trigger  \n", "   * in Storage account select your Storage account  \n", "   * in Resource group select your Resource group  \n", "   * Set the 'Create new Azure Blob Storage trigger' to your container name (used in the Dataloop platform)  \n", "    ![add_layer](../../../../assets/bind_azure/trigger_dataset.png)  \n", "   * open the code file  \n", "   * add dtlpy to the requirements.txt file  \n", "   * add **\"disabled\": false** to the function.json file  \n", "   * add a function code to \\_\\_init\\_\\_.py file  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import azure.functions as func\n", "import dtlpy as dl\n", "import os\n", "", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "dataset_id = os.environ.get('DATASET_ID')\n", "dtlpy_username = os.environ.get('DTLPY_USERNAME')\n", "dtlpy_password = os.environ.get('DTLPY_PASSWORD')\n", "", "def main(myblob: func.InputStream):\n", "    dl.login_m2m(email=dtlpy_username, password=dtlpy_password)\n", "    dataset = dl.datasets.get(dataset_id=dataset_id,\n", "                              fetch=False  # to avoid GET the dataset each time\n", "                              )\n", "", "    # remove th Container name from the path\n", "    path_parser = myblob.name.split('/')\n", "    file_name = '/'.join(path_parser[1:])\n", "", "    file_name = 'external://' + file_name\n", "    dataset.items.upload(local_path=file_name)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Deploy the code to the function app you created.  \n", "5. In VS code go to view tab -> Command Palette -> Azure Functions: Upload Local Settings  \n", "6. Go to the Function App -> Select your function -> Configuration (Under Settings section)  \n", "       * add the 3 secrets vars DATASET_ID, DTLPY_USERNAME, DTLPY_PASSWORD  \n", "  \n", "**Done! Now your storage blob will be synced with the Dataloop dataset**  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}